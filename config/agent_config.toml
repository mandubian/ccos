# RTFS Agent Configuration (strict runner)
version = "1"
agent_id = "demo-agent"
profile = "default"
features = ["delegation"]

[orchestrator.isolation]
enabled = false
mode = "none"

[orchestrator.dlp]
enabled = false

[network]
enabled = true

[network.egress]
via = "direct"
allow_domains = []
mtls = false

[capabilities.http]
enabled = false

[capabilities.http.egress]
allow_domains = []
mtls = false

[capabilities.fs]
enabled = false

[capabilities.llm]
enabled = true

[governance.keys]
verify = "dummy-verify"

# Simple policy shorthand: "allow" = low risk, no approvals, permissive budgets
# For custom limits, use the full structure with limits.* and policies.* subfields
[governance.policies]
default = "allow"

[causal_chain.storage]
mode = "in_memory" # or "append_file" | "sqlite" | "vsock_stream"
buffer_local = false

[causal_chain.anchor]
enabled = false

[marketplace]
registry_paths = []

# Storage paths configuration
# All paths are relative to the workspace root (directory containing config/)
[storage]
# Base directory for all capabilities
capabilities_dir = "capabilities"
# Subdirectory for generated/synthesized capabilities  
generated_subdir = "generated"
learned_subdir = "learned"
# Subdirectory for approved server capabilities
servers_approved_subdir = "servers/approved"
# Subdirectory for sessions
sessions_subdir = "sessions"

# LLM Validation Configuration
# Controls schema and plan validation using LLM
[validation]
# Enable LLM schema validation for inferred schemas
enable_schema_validation = true
# Enable LLM plan validation (schema compatibility, dependencies)
enable_plan_validation = true
# Enable auto-repair on validation failures
enable_auto_repair = true
# Enable LLM repair for runtime execution errors (dialog loop)
enable_runtime_repair = true
# Max auto-repair attempts before queuing for external review
max_repair_attempts = 2
# Max runtime repair attempts (dialog loop bound)
max_runtime_repair_attempts = 5
# LLM profile to use for validation (from llm_profiles section)
# Uses a lower quality/faster model for validation tasks
# Uses a lower quality/faster model for validation tasks
validation_profile = "openrouter_free:qwen3_coder_next"

# Real-time tracking and observability configuration
# Controls WebSocket streaming, heartbeat intervals, and event replay
[realtime_tracking]
# WebSocket ping interval in seconds (keepalive for connection)
ws_ping_interval_secs = 10

# Agent heartbeat timeout for "unhealthy" status (seconds)
heartbeat_timeout_secs = 3

# Agent heartbeat send interval (seconds) - how often agent reports status
heartbeat_send_interval_secs = 1

# Gateway health check interval (seconds) - how often gateway verifies agents
health_check_interval_secs = 5

# Number of historical events to replay on WebSocket connect
event_replay_limit = 100

# AI Self-Programming Configuration
# Controls autonomous capability creation and modification
[self_programming]
# Enable self-programming features (decompose, resolve, synthesize)
enabled = true
# Trust level (0-4): 0=all approval, 1=introspect ok, 2=transforms ok, 3=synth ok, 4=full
# Level 0: ALL self-modification requires approval (recommended for start)
# Level 1: Read-only introspection auto-approved
# Level 2: Pure data transforms auto-approved
# Level 3: Capability synthesis auto-approved (with rollback)
# Level 4: Full autonomy (use with extreme caution)
trust_level = 0
# Maximum synthesis attempts per session before requiring approval
max_synthesis_per_session = 10
# Maximum recursion depth for goal decomposition
max_decomposition_depth = 5
# Require explicit approval for capability registration (enforced at trust_level < 3)
require_approval_for_registration = true
# Enable capability versioning for rollback support
enable_versioning = true
# Auto-rollback on execution failure
auto_rollback_on_failure = true

# MCP Discovery Configuration
[mcp_discovery]
# Enable output schema introspection (calls tools with safe inputs)
introspect_output_schemas = false
# Use cache for discovered tools
use_cache = true
# Automatically register discovered tools in marketplace
register_in_marketplace = true
# Export discovered tools to RTFS files
export_to_rtfs = true
# Directory for exported RTFS files (relative to workspace root)
export_directory = "capabilities/servers/pending"

# Rate limiting configuration (token bucket algorithm)
[mcp_discovery.rate_limit]
# Requests per second (default: 10.0)
requests_per_second = 10.0
# Maximum burst size (default: 20)
burst_size = 20

# Retry policy configuration (exponential backoff)
[mcp_discovery.retry_policy]
# Maximum number of retries (default: 3)
max_retries = 3
# Initial delay in milliseconds (default: 1000)
initial_delay_ms = 1000
# Maximum delay in milliseconds (default: 30000)
max_delay_ms = 30000
# Exponential base for backoff (default: 2.0)
exponential_base = 2.0
# Add jitter to prevent thundering herd (default: true)
jitter = true

[delegation]
enabled = true
print_extracted_intent = true
print_extracted_plan = true

[discovery]
# Minimum semantic match score threshold (0.0 to 1.0)
# Higher values reduce false positives but may miss valid matches
match_threshold = 0.65

# Enable embedding-based matching (more accurate but requires API)
# Requires OPENROUTER_API_KEY or LOCAL_EMBEDDING_URL to be set
use_embeddings = true

# Preferred embedding models (optional overrides for env defaults)
# Remote/OpenRouter model to request (falls back to env EMBEDDING_MODEL)
embedding_model = "text-embedding-3-small"
# Local embedding model name when using LOCAL_EMBEDDING_URL
# local_embedding_model = "nomic-embed-text"
local_embedding_model = "embeddinggemma"

# Minimum score required for action verb match (0.0 to 1.0)
# Action verbs (display, filter, list, etc.) must match for capabilities to be considered compatible
action_verb_threshold = 0.7

# Weight for action verbs in matching (0.0 to 1.0)
# Higher = action verbs are more important in matching decisions
action_verb_weight = 0.4

# Weight for capability class matching (0.0 to 1.0)
# How much the capability class operation type (e.g., "list" in "ui.list.display") matters
capability_class_weight = 0.3

[catalog]
# Minimum semantic similarity score required to reuse a catalog plan hit
plan_min_score = 0.65
# Minimum keyword score required to reuse a catalog plan hit
keyword_min_score = 1.0

[missing_capabilities]
# Enable runtime detection of missing capabilities; falls back to env CCOS_MISSING_CAPABILITY_ENABLED when unset
enabled = true
runtime_detection = true
# Allow the resolver to auto-attempt repairs instead of deferring to the user
auto_resolution = true
# Enable output schema introspection (requires auth)
output_schema_introspection = true
# Let the resolver call the promptstore-backed LLM synthesis flow
llm_synthesis = true
# Enable web search discovery fallback (disabled by default for performance/privacy)
web_search = true
# Optional guardrails
human_approval_required = false
# Tweak attempt/verbosity knobs (env overrides still win)
max_attempts = 3
verbose_logging = false

[server_discovery_pipeline]
# Enable the pipeline globally
enabled = true
# Default execution mode: "preview" or "stage_and_queue"
mode = "stage_and_queue"
# Ordered stages for query discovery
query_pipeline_order = ["registry_search", "llm_suggest", "rank", "dedupe", "limit"]
# Ordered introspection stages (fallback order)
introspection_order = ["mcp", "openapi", "browser"]
# Candidate limits and threshold
max_candidates = 30
max_ranked = 15
threshold = 0.65

[server_discovery_pipeline.sources]
mcp_registry = true
npm = true
overrides = true
apis_guru = true
# If unset, falls back to missing_capabilities.web_search
# web_search = true
llm_suggest = true
known_apis = true

[server_discovery_pipeline.introspection]
mcp_http = true
mcp_stdio = true
openapi = true
browser = true

[server_discovery_pipeline.staging]
pending_subdir = "capabilities/servers/pending"
server_id_strategy = "sanitize_filename"
layout = "rtfs_layout_v1"

[server_discovery_pipeline.approvals]
enabled = true
expiry_hours = 168
risk_default = "medium"

[llm_profiles]
default = "openrouter:balanced_gfl"



# Stub profile - ONLY for testing (not realistic)
# To use: explicitly pass --profile stub/dev
[[llm_profiles.profiles]]
name = "stub/dev"
provider = "stub"
model = "stub-mini"
# Note: This profile should only be used for basic tests
# For realistic behavior, use openrouter:balanced (default) or openrouter_free:fast


# New: grouped model set
[[llm_profiles.model_sets]]
name = "lmstudio"
provider = "lmstudio"
api_key_env = "LMSTUDIO_API_KEY"
base_url = "http://192.168.1.19:1234"
default = "qwen_35_35_a10"

   [[llm_profiles.model_sets.models]]
    name = "qwen_35_35_a10"
    model = "qwen/qwen3.5-35b-a3b"
    max_prompt_cost_per_1k = 0.0
    max_completion_cost_per_1k = 0.0
    quality = "speed"
    notes = "Good latency / cheaper."

# New: grouped model set
[[llm_profiles.model_sets]]
name = "openrouter"
provider = "openrouter"
api_key_env = "OPENROUTER_API_KEY"
base_url = "https://openrouter.ai/api/v1"
default = "balanced_gfl"

  [[llm_profiles.model_sets.models]]
  name = "fast"
  model = "nvidia/nemotron-nano-9b-v2:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "speed"
  notes = "Good latency / cheaper."

  [[llm_profiles.model_sets.models]]
  name = "balanced"
  model = "deepseek/deepseek-v3.2-exp"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_m2"
  model = "minimax/minimax-m2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_gfl"
  model = "google/gemini-3-flash-preview"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_ds_32"
  model = "deepseek/deepseek-v3.2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_ds_32_spe"
  model = "deepseek/deepseek-v3.2-speciale"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"  

  [[llm_profiles.model_sets.models]]
  name = "step_35_flash"
  model = "stepfun/step-3.5-flash:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "quality"
  notes = "StepFun: Step 3.5 Flash (free)."

  [[llm_profiles.model_sets.models]]
  name = "qwen3_coder_next"
  model = "qwen/qwen3-coder-next"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "quality"
  notes = "Qwen3 Coder Next."

  
  [[llm_profiles.model_sets.models]]
  name = "minimax_25"
  model = "minimax/minimax-m2.5"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "quality"
  notes = "Minimax 2.5."


# Existing explicit profiles still work
# [[llm_profiles.profiles]]
# name = "fast"
# provider = "openrouter"
# model = "meta-llama/llama-3-8b-instruct"
# api_key_env = "OPENROUTER_API_KEY"

# =============================================================================
# SANDBOX CONFIGURATION (Phase 2)
# =============================================================================
# Controls code execution sandbox behavior, package management, and caching

[sandbox]
# Enable sandbox execution
enabled = true
# Runtime type: "bubblewrap" (Linux only), "docker", or "microvm"
runtime = "bubblewrap"
# Package cache directory
package_cache_dir = "/tmp/ccos-sandbox-cache"
# Enable package caching for faster subsequent installs
enable_package_cache = true

# Package Allowlist Configuration
[sandbox.package_allowlist]
# Auto-approved packages (no approval required)
auto_approved = [
    "pandas",
    "numpy",
    "matplotlib",
    "seaborn",
    "scipy",
    "scikit-learn",
    "requests",
    "beautifulsoup4",
    "pillow",
    "openpyxl",
    "xlrd",
    "pyyaml",
    "jinja2",
    "lxml",
    "httpx",
    "aiohttp",
]
# Packages requiring explicit approval
requires_approval = [
    "torch",
    "tensorflow",
    "jax",
    "langchain",
    "openai",
    "transformers",
    "datasets",
]
# Blocked packages (security risk)
blocked = [
    "subprocess32",
    "pyautogui",
    "keyboard",
    "mouse",
    "pynput",
]

# Pre-baked Sandbox Images
[[sandbox.images]]
name = "python-data-science"
base = "python:3.12-slim"
packages = [
    "pandas>=2.0",
    "numpy>=1.26",
    "matplotlib>=3.8",
    "seaborn>=0.13",
    "scipy>=1.11",
    "scikit-learn>=1.3",
    "requests>=2.31",
]
warm_pool_size = 0  # Disabled for now (cold start)

[[sandbox.images]]
name = "python-web"
base = "python:3.12-slim"
packages = [
    "requests>=2.31",
    "httpx>=0.25",
    "beautifulsoup4>=4.12",
    "lxml>=4.9",
    "aiohttp>=3.9",
]
warm_pool_size = 0

# =============================================================================
# CODING AGENTS CONFIGURATION (Phase 3)
# =============================================================================
# Controls specialized code generation with dedicated LLM profiles
# Supports multi-turn conversations and automated test generation

[coding_agents]
# Default coding agent profile to use
# default = "deepseek-coder"
default = "gemini-flash"

# Enable automatic test generation
# When true, coding agents will generate unit tests alongside code
generate_tests = true

# Maximum conversation turns for multi-turn coding
# Allows iterative refinement: "refactor this", "add error handling", etc.
max_conversation_turns = 5

# Coding Agent Profiles
# Define specialized LLM configurations for code generation

[[coding_agents.profiles]]
name = "deepseek-coder"
provider = "openrouter"
model = "deepseek/deepseek-coder-v2-instruct"
api_key_env = "OPENROUTER_API_KEY"
system_prompt = """
You are an expert code generation assistant. Generate clean, well-documented code.

Always provide:
1. Import statements
2. Type hints (Python) or TypeScript types (JS)
3. Error handling
4. Brief comments explaining logic
5. Unit tests with good coverage

Response format (JSON):
{
  "code": "actual code here",
  "language": "python",
  "dependencies": ["pandas", "numpy"],
  "explanation": "what this code does",
  "tests": "unit test code"
}
"""
max_tokens = 4096
temperature = 0.2

[[coding_agents.profiles]]
name = "claude-sonnet"
provider = "anthropic"
model = "claude-3-sonnet-20240229"
api_key_env = "ANTHROPIC_API_KEY"
system_prompt = """
You are an expert programmer. Write production-quality code.
Focus on correctness, efficiency, and maintainability.
Always validate inputs and handle edge cases.
Include comprehensive unit tests.

Response format (JSON):
{
  "code": "actual code here",
  "language": "python",
  "dependencies": ["pandas", "numpy"],
  "explanation": "what this code does",
  "tests": "unit test code"
}
"""
max_tokens = 8192
temperature = 0.3

[[coding_agents.profiles]]
name = "gemini-flash"
provider = "openrouter"
model = "google/gemini-3-flash-preview"
api_key_env = "OPENROUTER_API_KEY"
system_prompt = """
You are an expert code generation assistant. Generate clean, well-documented code.

Always provide:
1. Import statements
2. Type hints (Python) or TypeScript types (JS)
3. Error handling
4. Brief comments explaining logic
5. Unit tests with good coverage

When fetching data from URLs (requests, feedparser, etc.), ALWAYS set a browser-like User-Agent header to avoid being blocked.

Response format (JSON):
{
  "code": "actual code here",
  "language": "python",
  "dependencies": ["pandas", "numpy"],
  "explanation": "what this code does",
  "tests": "unit test code"
}
"""
max_tokens = 8192
temperature = 0.2

[[coding_agents.profiles]]
name = "gpt-4"
provider = "openai"
model = "gpt-4-turbo-preview"
api_key_env = "OPENAI_API_KEY"
system_prompt = """
You are a senior software engineer. Generate high-quality, maintainable code
with excellent documentation and comprehensive test coverage.

Response format (JSON):
{
  "code": "actual code here",
  "language": "python",
  "dependencies": ["pandas", "numpy"],
  "explanation": "what this code does",
  "tests": "unit test code"
}
"""
max_tokens = 4096
temperature = 0.2

# =============================================================================
# AUTONOMOUS AGENT CONFIGURATION
# =============================================================================
# Controls iterative LLM consultation and autonomous action execution

[autonomous_agent]
# Enable iterative mode (agent consults LLM after each action)
enabled = true

# Maximum iterations per user request (safety limit)
max_iterations = 10

# Maximum context entries to keep in conversation history
# When exceeded, older entries are handled according to context_strategy
max_context_entries = 20

# Context management strategy when max_context_entries exceeded:
# - "truncate": Remove oldest entries
# - "summarize": Compress old entries into summary (not yet implemented, falls back to truncate)
context_strategy = "truncate"

# Enable intermediate progress responses to user
# When true, agent sends updates after each action
send_intermediate_responses = false

# On action failure: "ask_user" or "abort"
failure_handling = "ask_user"

# Max consecutive failures before asking user (prevents infinite loops)
max_consecutive_failures = 2
