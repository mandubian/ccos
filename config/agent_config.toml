# RTFS Agent Configuration (strict runner)
version = "1"
agent_id = "demo-agent"
profile = "default"
features = ["delegation"]

[orchestrator.isolation]
enabled = false
mode = "none"

[orchestrator.dlp]
enabled = false

[network]
enabled = true

[network.egress]
via = "direct"
allow_domains = []
mtls = false

[capabilities.http]
enabled = false

[capabilities.http.egress]
allow_domains = []
mtls = false

[capabilities.fs]
enabled = false

[capabilities.llm]
enabled = true

[governance.keys]
verify = "dummy-verify"

[governance.policies.default]
risk_tier = "low"
requires_approvals = 0

[governance.policies.default.budgets]
max_cost_usd = 1.0
token_budget = 5000

[causal_chain.storage]
mode = "in_memory" # or "append_file" | "sqlite" | "vsock_stream"
buffer_local = false

[causal_chain.anchor]
enabled = false

[marketplace]
registry_paths = []

# Storage paths configuration
# All paths are relative to this config file's directory (config/)
[storage]
# Base directory for all capabilities
capabilities_dir = "../capabilities"
# Subdirectory for discovered capabilities
discovered_subdir = "discovered"
# Subdirectory for generated/synthesized capabilities  
generated_subdir = "generated"
# Subdirectory for approved server capabilities
servers_approved_subdir = "servers/approved"

# LLM Validation Configuration
# Controls schema and plan validation using LLM
[validation]
# Enable LLM schema validation for inferred schemas
enable_schema_validation = true
# Enable LLM plan validation (schema compatibility, dependencies)
enable_plan_validation = true
# Enable auto-repair on validation failures
enable_auto_repair = true
# Max auto-repair attempts before queuing for external review
max_repair_attempts = 2
# LLM profile to use for validation (from llm_profiles section)
# Uses a lower quality/faster model for validation tasks
validation_profile = "openrouter_free:balanced_gfl"

# MCP Discovery Configuration
[mcp_discovery]
# Enable output schema introspection (calls tools with safe inputs)
introspect_output_schemas = false
# Use cache for discovered tools
use_cache = true
# Automatically register discovered tools in marketplace
register_in_marketplace = true
# Export discovered tools to RTFS files
export_to_rtfs = true
# Directory for exported RTFS files (relative to workspace root)
export_directory = "capabilities/discovered"

# Rate limiting configuration (token bucket algorithm)
[mcp_discovery.rate_limit]
# Requests per second (default: 10.0)
requests_per_second = 10.0
# Maximum burst size (default: 20)
burst_size = 20

# Retry policy configuration (exponential backoff)
[mcp_discovery.retry_policy]
# Maximum number of retries (default: 3)
max_retries = 3
# Initial delay in milliseconds (default: 1000)
initial_delay_ms = 1000
# Maximum delay in milliseconds (default: 30000)
max_delay_ms = 30000
# Exponential base for backoff (default: 2.0)
exponential_base = 2.0
# Add jitter to prevent thundering herd (default: true)
jitter = true

[delegation]
enabled = true
print_extracted_intent = true
print_extracted_plan = true

[discovery]
# Minimum semantic match score threshold (0.0 to 1.0)
# Higher values reduce false positives but may miss valid matches
match_threshold = 0.65

# Enable embedding-based matching (more accurate but requires API)
# Requires OPENROUTER_API_KEY or LOCAL_EMBEDDING_URL to be set
use_embeddings = true

# Preferred embedding models (optional overrides for env defaults)
# Remote/OpenRouter model to request (falls back to env EMBEDDING_MODEL)
embedding_model = "text-embedding-3-small"
# Local embedding model name when using LOCAL_EMBEDDING_URL
# local_embedding_model = "nomic-embed-text"
local_embedding_model = "embeddinggemma"

# Minimum score required for action verb match (0.0 to 1.0)
# Action verbs (display, filter, list, etc.) must match for capabilities to be considered compatible
action_verb_threshold = 0.7

# Weight for action verbs in matching (0.0 to 1.0)
# Higher = action verbs are more important in matching decisions
action_verb_weight = 0.4

# Weight for capability class matching (0.0 to 1.0)
# How much the capability class operation type (e.g., "list" in "ui.list.display") matters
capability_class_weight = 0.3

[catalog]
# Minimum semantic similarity score required to reuse a catalog plan hit
plan_min_score = 0.65
# Minimum keyword score required to reuse a catalog plan hit
keyword_min_score = 1.0

[missing_capabilities]
# Enable runtime detection of missing capabilities; falls back to env CCOS_MISSING_CAPABILITY_ENABLED when unset
enabled = true
runtime_detection = true
# Allow the resolver to auto-attempt repairs instead of deferring to the user
auto_resolution = true
# Enable output schema introspection (requires auth)
output_schema_introspection = true
# Let the resolver call the promptstore-backed LLM synthesis flow
llm_synthesis = true
# Enable web search discovery fallback (disabled by default for performance/privacy)
web_search = false
# Optional guardrails
human_approval_required = false
# Tweak attempt/verbosity knobs (env overrides still win)
max_attempts = 3
verbose_logging = false

[llm_profiles]
default = "openrouter_free:balanced"

# Stub profile - ONLY for testing (not realistic)
# To use: explicitly pass --profile stub/dev
[[llm_profiles.profiles]]
name = "stub/dev"
provider = "stub"
model = "stub-mini"
# Note: This profile should only be used for basic tests
# For realistic behavior, use openrouter_free:balanced (default) or openrouter_free:fast


# New: grouped model set
[[llm_profiles.model_sets]]
name = "openrouter_free"
provider = "openrouter"
api_key_env = "OPENROUTER_API_KEY"
base_url = "https://openrouter.ai/api/v1"
default = "balanced_gfl"

  [[llm_profiles.model_sets.models]]
  name = "fast"
  model = "nvidia/nemotron-nano-9b-v2:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "speed"
  notes = "Good latency / cheaper."

  [[llm_profiles.model_sets.models]]
  name = "balanced"
  model = "deepseek/deepseek-v3.2-exp"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_m2"
  model = "minimax/minimax-m2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_gfl"
  model = "google/gemini-2.5-flash-lite"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  max_tokens = 4096
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_ds_32"
  model = "deepseek/deepseek-v3.2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_ds_32_spe"
  model = "deepseek/deepseek-v3.2-speciale"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"  

  [[llm_profiles.model_sets.models]]
  name = "premium"
  model = "x-ai/grok-4-fast:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "quality"
  notes = "Higher reasoning depth."

# Existing explicit profiles still work
[[llm_profiles.profiles]]
name = "fast"
provider = "openrouter"
model = "meta-llama/llama-3-8b-instruct"
api_key_env = "OPENROUTER_API_KEY"