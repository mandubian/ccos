# RTFS Agent Configuration (strict runner)
version = "1"
agent_id = "demo-agent"
profile = "default"
features = ["delegation"]

[orchestrator.isolation]
enabled = false
mode = "none"

[orchestrator.dlp]
enabled = false

[network]
enabled = true

[network.egress]
via = "direct"
allow_domains = []
mtls = false

[capabilities.http]
enabled = false

[capabilities.http.egress]
allow_domains = []
mtls = false

[capabilities.fs]
enabled = false

[capabilities.llm]
enabled = true

[governance.keys]
verify = "dummy-verify"

# Simple policy shorthand: "allow" = low risk, no approvals, permissive budgets
# For custom limits, use the full structure with limits.* and policies.* subfields
[governance.policies]
default = "allow"

[causal_chain.storage]
mode = "in_memory" # or "append_file" | "sqlite" | "vsock_stream"
buffer_local = false

[causal_chain.anchor]
enabled = false

[marketplace]
registry_paths = []

# Storage paths configuration
# All paths are relative to the workspace root (directory containing config/)
[storage]
# Base directory for all capabilities
capabilities_dir = "capabilities"
# Subdirectory for generated/synthesized capabilities  
generated_subdir = "generated"
# Subdirectory for approved server capabilities
servers_approved_subdir = "servers/approved"
# Subdirectory for sessions
sessions_subdir = "sessions"

# LLM Validation Configuration
# Controls schema and plan validation using LLM
[validation]
# Enable LLM schema validation for inferred schemas
enable_schema_validation = true
# Enable LLM plan validation (schema compatibility, dependencies)
enable_plan_validation = true
# Enable auto-repair on validation failures
enable_auto_repair = true
# Enable LLM repair for runtime execution errors (dialog loop)
enable_runtime_repair = true
# Max auto-repair attempts before queuing for external review
max_repair_attempts = 2
# Max runtime repair attempts (dialog loop bound)
max_runtime_repair_attempts = 5
# LLM profile to use for validation (from llm_profiles section)
# Uses a lower quality/faster model for validation tasks
validation_profile = "openrouter_free:balanced_gfl"

# AI Self-Programming Configuration
# Controls autonomous capability creation and modification
[self_programming]
# Enable self-programming features (decompose, resolve, synthesize)
enabled = true
# Trust level (0-4): 0=all approval, 1=introspect ok, 2=transforms ok, 3=synth ok, 4=full
# Level 0: ALL self-modification requires approval (recommended for start)
# Level 1: Read-only introspection auto-approved
# Level 2: Pure data transforms auto-approved
# Level 3: Capability synthesis auto-approved (with rollback)
# Level 4: Full autonomy (use with extreme caution)
trust_level = 0
# Maximum synthesis attempts per session before requiring approval
max_synthesis_per_session = 10
# Maximum recursion depth for goal decomposition
max_decomposition_depth = 5
# Require explicit approval for capability registration (enforced at trust_level < 3)
require_approval_for_registration = true
# Enable capability versioning for rollback support
enable_versioning = true
# Auto-rollback on execution failure
auto_rollback_on_failure = true

# MCP Discovery Configuration
[mcp_discovery]
# Enable output schema introspection (calls tools with safe inputs)
introspect_output_schemas = false
# Use cache for discovered tools
use_cache = true
# Automatically register discovered tools in marketplace
register_in_marketplace = true
# Export discovered tools to RTFS files
export_to_rtfs = true
# Directory for exported RTFS files (relative to workspace root)
export_directory = "capabilities/servers/pending"

# Rate limiting configuration (token bucket algorithm)
[mcp_discovery.rate_limit]
# Requests per second (default: 10.0)
requests_per_second = 10.0
# Maximum burst size (default: 20)
burst_size = 20

# Retry policy configuration (exponential backoff)
[mcp_discovery.retry_policy]
# Maximum number of retries (default: 3)
max_retries = 3
# Initial delay in milliseconds (default: 1000)
initial_delay_ms = 1000
# Maximum delay in milliseconds (default: 30000)
max_delay_ms = 30000
# Exponential base for backoff (default: 2.0)
exponential_base = 2.0
# Add jitter to prevent thundering herd (default: true)
jitter = true

[delegation]
enabled = true
print_extracted_intent = true
print_extracted_plan = true

[discovery]
# Minimum semantic match score threshold (0.0 to 1.0)
# Higher values reduce false positives but may miss valid matches
match_threshold = 0.65

# Enable embedding-based matching (more accurate but requires API)
# Requires OPENROUTER_API_KEY or LOCAL_EMBEDDING_URL to be set
use_embeddings = true

# Preferred embedding models (optional overrides for env defaults)
# Remote/OpenRouter model to request (falls back to env EMBEDDING_MODEL)
embedding_model = "text-embedding-3-small"
# Local embedding model name when using LOCAL_EMBEDDING_URL
# local_embedding_model = "nomic-embed-text"
local_embedding_model = "embeddinggemma"

# Minimum score required for action verb match (0.0 to 1.0)
# Action verbs (display, filter, list, etc.) must match for capabilities to be considered compatible
action_verb_threshold = 0.7

# Weight for action verbs in matching (0.0 to 1.0)
# Higher = action verbs are more important in matching decisions
action_verb_weight = 0.4

# Weight for capability class matching (0.0 to 1.0)
# How much the capability class operation type (e.g., "list" in "ui.list.display") matters
capability_class_weight = 0.3

[catalog]
# Minimum semantic similarity score required to reuse a catalog plan hit
plan_min_score = 0.65
# Minimum keyword score required to reuse a catalog plan hit
keyword_min_score = 1.0

[missing_capabilities]
# Enable runtime detection of missing capabilities; falls back to env CCOS_MISSING_CAPABILITY_ENABLED when unset
enabled = true
runtime_detection = true
# Allow the resolver to auto-attempt repairs instead of deferring to the user
auto_resolution = true
# Enable output schema introspection (requires auth)
output_schema_introspection = true
# Let the resolver call the promptstore-backed LLM synthesis flow
llm_synthesis = true
# Enable web search discovery fallback (disabled by default for performance/privacy)
web_search = true
# Optional guardrails
human_approval_required = false
# Tweak attempt/verbosity knobs (env overrides still win)
max_attempts = 3
verbose_logging = false

[server_discovery_pipeline]
# Enable the pipeline globally
enabled = true
# Default execution mode: "preview" or "stage_and_queue"
mode = "stage_and_queue"
# Ordered stages for query discovery
query_pipeline_order = ["registry_search", "llm_suggest", "rank", "dedupe", "limit"]
# Ordered introspection stages (fallback order)
introspection_order = ["mcp", "openapi", "browser"]
# Candidate limits and threshold
max_candidates = 30
max_ranked = 15
threshold = 0.65

[server_discovery_pipeline.sources]
mcp_registry = true
npm = true
overrides = true
apis_guru = true
# If unset, falls back to missing_capabilities.web_search
# web_search = true
llm_suggest = true
known_apis = true

[server_discovery_pipeline.introspection]
mcp_http = true
mcp_stdio = true
openapi = true
browser = true

[server_discovery_pipeline.staging]
pending_subdir = "capabilities/servers/pending"
server_id_strategy = "sanitize_filename"
layout = "rtfs_layout_v1"

[server_discovery_pipeline.approvals]
enabled = true
expiry_hours = 168
risk_default = "medium"

[llm_profiles]
default = "openrouter_free:balanced_ds_32"

# Google Gemini profile
[[llm_profiles.model_sets]]
name = "google"
provider = "google"
api_key_env = "GEMINI_API_KEY"
default = "gemini-1.5-pro"

  [[llm_profiles.model_sets.models]]
  name = "gemini-1.5-pro"
  model = "gemini-1.5-pro"
  quality = "balanced"

# Stub profile - ONLY for testing (not realistic)
# To use: explicitly pass --profile stub/dev
[[llm_profiles.profiles]]
name = "stub/dev"
provider = "stub"
model = "stub-mini"
# Note: This profile should only be used for basic tests
# For realistic behavior, use openrouter_free:balanced (default) or openrouter_free:fast


# New: grouped model set
[[llm_profiles.model_sets]]
name = "openrouter_free"
provider = "openrouter"
api_key_env = "OPENROUTER_API_KEY"
base_url = "https://openrouter.ai/api/v1"
default = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "fast"
  model = "nvidia/nemotron-nano-9b-v2:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "speed"
  notes = "Good latency / cheaper."

  [[llm_profiles.model_sets.models]]
  name = "balanced"
  model = "deepseek/deepseek-v3.2-exp"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_m2"
  model = "minimax/minimax-m2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_gfl"
  model = "google/gemini-2.5-flash-lite"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  max_tokens = 4096
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_ds_32"
  model = "deepseek/deepseek-v3.2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_ds_32_spe"
  model = "deepseek/deepseek-v3.2-speciale"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"  

  [[llm_profiles.model_sets.models]]
  name = "premium"
  model = "x-ai/grok-4-fast:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "quality"
  notes = "Higher reasoning depth."

# Existing explicit profiles still work
[[llm_profiles.profiles]]
name = "fast"
provider = "openrouter"
model = "meta-llama/llama-3-8b-instruct"
api_key_env = "OPENROUTER_API_KEY"