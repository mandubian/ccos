# RTFS Agent Configuration (strict runner)
version = "1"
agent_id = "demo-agent"
profile = "default"
features = ["delegation"]

[orchestrator.isolation]
enabled = false
mode = "none"

[orchestrator.dlp]
enabled = false

[network]
enabled = true

[network.egress]
via = "direct"
allow_domains = []
mtls = false

[capabilities.http]
enabled = false

[capabilities.http.egress]
allow_domains = []
mtls = false

[capabilities.fs]
enabled = false

[capabilities.llm]
enabled = true

[governance.keys]
verify = "dummy-verify"

[governance.policies.default]
risk_tier = "low"
requires_approvals = 0

[governance.policies.default.budgets]
max_cost_usd = 1.0
token_budget = 5000

[causal_chain.storage]
mode = "in_memory" # or "append_file" | "sqlite" | "vsock_stream"
buffer_local = false

[causal_chain.anchor]
enabled = false

[marketplace]
registry_paths = []

[delegation]
enabled = true
print_extracted_intent = true
print_extracted_plan = true

[discovery]
# Minimum semantic match score threshold (0.0 to 1.0)
# Higher values reduce false positives but may miss valid matches
match_threshold = 0.65

# Enable embedding-based matching (more accurate but requires API)
# Requires OPENROUTER_API_KEY or LOCAL_EMBEDDING_URL to be set
use_embeddings = true

# Preferred embedding models (optional overrides for env defaults)
# Remote/OpenRouter model to request (falls back to env EMBEDDING_MODEL)
embedding_model = "text-embedding-3-small"
# Local embedding model name when using LOCAL_EMBEDDING_URL
# local_embedding_model = "nomic-embed-text"
local_embedding_model = "embeddinggemma"

# Minimum score required for action verb match (0.0 to 1.0)
# Action verbs (display, filter, list, etc.) must match for capabilities to be considered compatible
action_verb_threshold = 0.7

# Weight for action verbs in matching (0.0 to 1.0)
# Higher = action verbs are more important in matching decisions
action_verb_weight = 0.4

# Weight for capability class matching (0.0 to 1.0)
# How much the capability class operation type (e.g., "list" in "ui.list.display") matters
capability_class_weight = 0.3

[catalog]
# Minimum semantic similarity score required to reuse a catalog plan hit
plan_min_score = 0.65
# Minimum keyword score required to reuse a catalog plan hit
keyword_min_score = 1.0

[missing_capabilities]
# Enable runtime detection of missing capabilities; falls back to env CCOS_MISSING_CAPABILITY_ENABLED when unset
enabled = true
runtime_detection = true
# Allow the resolver to auto-attempt repairs instead of deferring to the user
auto_resolution = true
# Let the resolver call the promptstore-backed LLM synthesis flow
llm_synthesis = true
# Optional guardrails
human_approval_required = false
# Tweak attempt/verbosity knobs (env overrides still win)
max_attempts = 3
verbose_logging = false

[llm_profiles]
default = "openrouter_free:balanced"

# Stub profile - ONLY for testing (not realistic)
# To use: explicitly pass --profile stub/dev
[[llm_profiles.profiles]]
name = "stub/dev"
provider = "stub"
model = "stub-mini"
# Note: This profile should only be used for basic tests
# For realistic behavior, use openrouter_free:balanced (default) or openrouter_free:fast


# New: grouped model set
[[llm_profiles.model_sets]]
name = "openrouter_free"
provider = "openrouter"
api_key_env = "OPENROUTER_API_KEY"
base_url = "https://openrouter.ai/api/v1"
default = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "fast"
  model = "nvidia/nemotron-nano-9b-v2:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "speed"
  notes = "Good latency / cheaper."

  [[llm_profiles.model_sets.models]]
  name = "balanced"
  model = "deepseek/deepseek-v3.2-exp"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_m2"
  model = "minimax/minimax-m2"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "balanced"

  [[llm_profiles.model_sets.models]]
  name = "balanced_gfl"
  model = "google/gemini-2.5-flash-lite"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  max_tokens = 4096
  quality = "balanced"


  [[llm_profiles.model_sets.models]]
  name = "premium"
  model = "x-ai/grok-4-fast:free"
  max_prompt_cost_per_1k = 0.0
  max_completion_cost_per_1k = 0.0
  quality = "quality"
  notes = "Higher reasoning depth."

# Existing explicit profiles still work
[[llm_profiles.profiles]]
name = "fast"
provider = "openrouter"
model = "meta-llama/llama-3-8b-instruct"
api_key_env = "OPENROUTER_API_KEY"